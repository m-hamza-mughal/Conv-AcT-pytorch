{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Training Notebook"]},{"cell_type":"markdown","metadata":{},"source":["we first need to build torchvision from source as it allows us to use ffmpeg and video_reader backend"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import distutils\n","distutils.spawn.find_executable('ffmpeg')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!apt install libavformat-dev libavcodec-dev libavdevice-dev libavutil-dev libswscale-dev libavresample-dev libavfilter-dev -y\n","!pip uninstall torchvision -y\n","!rm -r vision/\n","!git clone https://github.com/pytorch/vision.git\n","%cd vision\n","!python setup.py install\n","%cd /kaggle/working"]},{"cell_type":"markdown","metadata":{},"source":["## Restart but not reset"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-07-11T13:23:17.982462Z","iopub.status.busy":"2022-07-11T13:23:17.981937Z","iopub.status.idle":"2022-07-11T13:23:26.972724Z","shell.execute_reply":"2022-07-11T13:23:26.971597Z","shell.execute_reply.started":"2022-07-11T13:23:17.982340Z"},"trusted":true},"outputs":[],"source":["!pip show torchvision"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-07-11T13:23:32.673643Z","iopub.status.busy":"2022-07-11T13:23:32.672929Z","iopub.status.idle":"2022-07-11T13:23:33.543809Z","shell.execute_reply":"2022-07-11T13:23:33.542517Z","shell.execute_reply.started":"2022-07-11T13:23:32.673599Z"},"trusted":true},"outputs":[],"source":["import torchvision\n","import torch\n","from torch import nn, Tensor\n","from torchvision import models\n","import math\n","import gc\n","import random\n","\n","\n","random.seed(0)\n","torch.manual_seed(0)\n","DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') \n","torch.cuda.empty_cache()\n","torchvision.set_video_backend('video_reader')"]},{"cell_type":"markdown","metadata":{},"source":["## Modelling Pipeline"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2022-07-11T13:38:29.632648Z","iopub.status.busy":"2022-07-11T13:38:29.632126Z","iopub.status.idle":"2022-07-11T13:38:32.093296Z","shell.execute_reply":"2022-07-11T13:38:32.092099Z","shell.execute_reply.started":"2022-07-11T13:38:29.632612Z"},"trusted":true},"outputs":[],"source":["class PatchEmbedding(nn.Module):\n","    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n","        super(PatchEmbedding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n","\n","        position = torch.arange(max_len).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n","        pe = torch.zeros(1, max_len, d_model)\n","        pe[0, :, 0::2] = torch.sin(position * div_term)\n","        pe[0, :, 1::2] = torch.cos(position * div_term)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        \"\"\"\n","        Args:\n","            x: Tensor, shape [batch_size, n_frames, embedding_dim]\n","        \"\"\"\n","        b, _, _ = x.shape\n","        cls_tokens = self.cls_token.expand(b, -1, -1)\n","        x = torch.cat([cls_tokens, x], dim=1)\n","        x = x + self.pe[:, :x.size(1), :]\n","        return self.dropout(x)\n","\n","class FeatureExtractor(nn.Module):\n","    def __init__(self, d_model: int, model_name: str, model_weights: str = 'DEFAULT'):\n","        super(FeatureExtractor, self).__init__()\n","        assert model_name in ['efficientnet_v2_s', 'efficientnet_v2_m', 'efficientnet_v2_l', 'inception_v3', 'wide_resnet50_2', 'wide_resnet101_2']\n","        self.model = getattr(models, model_name)(weights=model_weights)\n","        \n","        if model_name in ['efficientnet_v2_s', 'efficientnet_v2_m', 'efficientnet_v2_l']:\n","            self.model.classifier = nn.Linear(in_features=self.model.classifier[1].in_features, out_features = d_model)\n","        else:\n","            self.model.fc = nn.Linear(in_features=self.model.fc.in_features ,out_features=d_model)\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        \"\"\"\n","        Args:\n","            x: Tensor, shape [batch_size, n_frames, channels, height, width]\n","        \"\"\"\n","        b, f, _, _, _ = x.shape\n","        #x = x.view(b*f, *x.size()[2:])\n","        features = [self.model(x[:, i]).unsqueeze(1) for i in range(f)]\n","        x = torch.cat(features, dim=1)\n","        assert x.size(1) == f\n","        # x = self.model(x)\n","        # x = x.view(b, f, *x.size()[1:])\n","\n","        return x\n","\n","\n","class ConvAcTransformer(nn.Module):\n","    def __init__(self, d_model: int,\n","                 attention_heads: int,\n","                 num_layers: int,\n","                 num_classes: int,\n","                 feature_extractor_name: str):\n","        super(ConvAcTransformer, self).__init__()\n","        self.d_model = d_model\n","        self.feature_extractor_name = feature_extractor_name\n","        self.attention_heads = attention_heads\n","        self.num_classes = num_classes\n","        self.num_layers = num_layers\n","\n","        \n","        self.feature_extract = FeatureExtractor(self.d_model, self.feature_extractor_name, model_weights=None)\n","        self.patch_embed = PatchEmbedding(self.d_model)\n","\n","        transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=self.d_model,\n","                                                                nhead=self.attention_heads,\n","                                                                norm_first=True,\n","                                                                activation='gelu')\n","        self.transformer_encoder = nn.TransformerEncoder(transformer_encoder_layer,\n","                                                         self.num_layers,\n","                                                         norm=nn.LayerNorm(self.d_model))\n","\n","        self.classification_head = nn.Linear(self.d_model, self.num_classes)\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        \"\"\"\n","        Args:\n","            x: Tensor, shape [batch_size, n_frames, channels, height, width]\n","        \"\"\"\n","        # extract features from all frames\n","        x = self.feature_extract(x)\n","\n","        # apply patch embedding from ViT\n","        x = self.patch_embed(x)\n","\n","        # ViT encoder\n","        x = self.transformer_encoder(x)\n","\n","        # select first token/classifier token\n","        x = x[:, 0, :]\n","\n","        # classification head\n","        x = self.classification_head(x)\n","\n","        return x\n","\n","\n","# test\n","test_tensor = torch.randn(16, 15, 3, 128, 128, dtype=torch.float32).to(DEVICE)\n","model = ConvAcTransformer(d_model=256, attention_heads=2, num_layers=2, num_classes=50, feature_extractor_name='efficientnet_v2_s')\n","model = model.to(device=DEVICE)\n","out = model(test_tensor)\n","print(out.size())\n","diff = out.mean().backward()\n","print(\"done\")\n","del test_tensor, out, diff\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["a = torch.randn(16,15,3,128,128)\n","a[:, 0].size()"]},{"cell_type":"markdown","metadata":{},"source":["## Data Pipeline"]},{"cell_type":"markdown","metadata":{},"source":["#### Data download using next cell: (comment if it's already there)"]},{"cell_type":"markdown","metadata":{},"source":["UCF-101 Download:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# !apt install unrar \n","# !wget https://www.crcv.ucf.edu/data/UCF101/UCF101.rar --no-check-certificate\n","# !wget https://www.crcv.ucf.edu/data/UCF101/UCF101TrainTestSplits-RecognitionTask.zip --no-check-certificate\n","# !unrar  x UCF101.rar -idq\n","# !unzip UCF101TrainTestSplits-RecognitionTask.zip\n","# ! rm UCF101.rar UCF101TrainTestSplits-RecognitionTask.zip"]},{"cell_type":"markdown","metadata":{},"source":["UCF-50 Download:"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-07-11T13:24:03.401346Z","iopub.status.busy":"2022-07-11T13:24:03.400413Z","iopub.status.idle":"2022-07-11T13:27:25.953135Z","shell.execute_reply":"2022-07-11T13:27:25.951893Z","shell.execute_reply.started":"2022-07-11T13:24:03.401304Z"},"trusted":true},"outputs":[],"source":["# !apt install unrar \n","# !wget https://www.crcv.ucf.edu/data/UCF50.rar --no-check-certificate\n","# !unrar x UCF50.rar -idq\n","# !mkdir ./ucf50TrainTestlist\n","# !wget -P ./ucf50TrainTestlist https://github.com/temur-kh/video-classification-cv/raw/master/data/classInd.txt\n","# !wget -O ./ucf50TrainTestlist/testlist01.txt https://github.com/temur-kh/video-classification-cv/raw/master/data/testlist.txt\n","# !wget -O ./ucf50TrainTestlist/trainlist01.txt https://github.com/temur-kh/video-classification-cv/raw/master/data/trainlist.txt\n","# ! rm UCF50.rar "]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-07-11T13:27:31.545208Z","iopub.status.busy":"2022-07-11T13:27:31.544804Z","iopub.status.idle":"2022-07-11T13:30:34.529573Z","shell.execute_reply":"2022-07-11T13:30:34.528371Z","shell.execute_reply.started":"2022-07-11T13:27:31.545171Z"},"trusted":true},"outputs":[],"source":["from torchvision.datasets import UCF101\n","from torchvision import transforms\n","\n","transforms = transforms.Compose([\n","    transforms.Lambda(lambda x: x.permute(0, 3, 1, 2)),\n","    transforms.Lambda(lambda x: x[::2]), # skip second frame\n","    transforms.RandomHorizontalFlip(p=0.5),\n","    # transforms.RandomResizedCrop(256, scale=(0.8, 1.0)),\n","    # transforms.CenterCrop(60),\n","    # transforms.RandomRotation(degrees=10, interpolation=transforms.InterpolationMode.BILINEAR),\n","    # transforms.GrayScale(),\n","    # transforms.GaussianBlur(kernel_size=3),\n","    # transforms.ColorJitter(brightness=.2, hue=.1),\n","    # transforms.RandomPerspective(distortion_scale=0.1),\n","    # transforms.AugMix(),\n","    # transforms.RandAugment(),\n","\n","    transforms.Lambda(lambda x: x / 255.),\n","    transforms.Lambda(lambda x: x.float()),\n","])\n","\n","train_dataset = UCF101(root = './UCF50/', annotation_path = './ucf50TrainTestlist/', transform=transforms ,_video_width=128, _video_height=128, train=True, frames_per_clip=30)\n","# valtest_dataset = UCF101(root = './UCF-101/', annotation_path = './ucfTrainTestlist/', transform=transforms ,_video_width=128, _video_height=128, train=False, frames_per_clip=30)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-07-11T13:39:13.230786Z","iopub.status.busy":"2022-07-11T13:39:13.230401Z","iopub.status.idle":"2022-07-11T13:39:13.237349Z","shell.execute_reply":"2022-07-11T13:39:13.236199Z","shell.execute_reply.started":"2022-07-11T13:39:13.230752Z"},"trusted":true},"outputs":[],"source":["# del model, x, y, loss\n","# gc.collect()\n","len(train_dataset.classes)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2022-07-11T13:39:14.072850Z","iopub.status.busy":"2022-07-11T13:39:14.072490Z","iopub.status.idle":"2022-07-11T13:39:14.136285Z","shell.execute_reply":"2022-07-11T13:39:14.135316Z","shell.execute_reply.started":"2022-07-11T13:39:14.072818Z"},"trusted":true},"outputs":[],"source":["\n","indices = random.sample(list(range(len(train_dataset))), len(train_dataset)//30)\n","\n","# Warp into Subsets\n","train_subset = torch.utils.data.Subset(train_dataset, indices[:-2000])\n","test_subset = torch.utils.data.Subset(train_dataset, indices[-1000:])\n","val_subset = torch.utils.data.Subset(train_dataset, indices[-2000:-1000])"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2022-07-11T13:39:15.006750Z","iopub.status.busy":"2022-07-11T13:39:15.006357Z","iopub.status.idle":"2022-07-11T13:39:15.023820Z","shell.execute_reply":"2022-07-11T13:39:15.022807Z","shell.execute_reply.started":"2022-07-11T13:39:15.006717Z"},"trusted":true},"outputs":[],"source":["def custom_collate(batch):\n","    # skip audio data\n","    filtered_batch = []\n","    for video, _, label in batch:\n","        filtered_batch.append((video, label))\n","    return torch.utils.data.dataloader.default_collate(filtered_batch)\n","\n","BATCH_SIZE = 16\n","train_loader = torch.utils.data.DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True,\n","                                           num_workers=2, pin_memory=True,\n","                                           collate_fn=custom_collate)\n","val_loader = torch.utils.data.DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=True,\n","                                           num_workers=2, pin_memory=True,\n","                                           collate_fn=custom_collate)\n","test_loader = torch.utils.data.DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True,\n","                                           num_workers=2,\n","                                           collate_fn=custom_collate)\n","\n","print(f\"Total number of train samples: {len(train_subset)}\")\n","print(f\"Total number of test samples: {len(test_subset)}\")\n","print(f\"Total number of val samples: {len(val_subset)}\")\n","print(f\"Total number of (train) batches: {len(train_loader)}\")\n","print(f\"Total number of (test) batches: {len(test_loader)}\")\n","print(f\"Total number of (val) batches: {len(val_loader)}\")"]},{"cell_type":"markdown","metadata":{},"source":["### Visualization"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2022-07-11T13:39:24.055751Z","iopub.status.busy":"2022-07-11T13:39:24.055393Z","iopub.status.idle":"2022-07-11T13:39:25.489459Z","shell.execute_reply":"2022-07-11T13:39:25.488446Z","shell.execute_reply.started":"2022-07-11T13:39:24.055712Z"},"trusted":true},"outputs":[],"source":["# !pip install imageio-ffmpeg\n","import imageio\n","from IPython.display import Video\n","import numpy as np\n","\n","for i, (fm,lb) in enumerate(train_loader):\n","    print(\"Dataset batch shape\", fm.size(), fm.dtype, lb,  lb.dtype )\n","    # break\n","    video = fm[0].permute(0, 2, 3, 1).numpy() \n","    # print(video.min(), video.max(), video.mean(), video.std())\n","    video = (video * 255.0).astype(np.uint8)\n","    label = train_dataset.classes[lb[0]]\n","    \n","    print(\"Example:\", label)\n","    imageio.mimwrite('./test.mp4', video, fps=30)\n","    break\n","\n","Video('./test.mp4', width=256, height=256, embed=True)"]},{"cell_type":"markdown","metadata":{},"source":["## Training Pipeline"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2022-07-11T13:42:41.707006Z","iopub.status.busy":"2022-07-11T13:42:41.706629Z","iopub.status.idle":"2022-07-11T13:42:42.956072Z","shell.execute_reply":"2022-07-11T13:42:42.954955Z","shell.execute_reply.started":"2022-07-11T13:42:41.706973Z"},"trusted":true},"outputs":[],"source":["torch.cuda.empty_cache()\n","!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-07-11T13:42:52.628535Z","iopub.status.busy":"2022-07-11T13:42:52.628153Z"},"trusted":true},"outputs":[],"source":["from torch import optim\n","from tqdm import tqdm\n","import copy\n","\n","# Defining model and training options\n","model = model.to(device=DEVICE)\n","\n","N_EPOCHS = 100\n","LR = 0.001\n","MODEL_PATH = 'best_model_ucf50_x128_sub_30'\n","best_val_acc = 0\n","best_model = None\n","pretrain = False\n","\n","# Training loop\n","optimizer = optim.AdamW(model.parameters(), lr=LR) #, weight_decay=0.1)\n","criterion = nn.CrossEntropyLoss(reduction='mean')\n","\n","if pretrain:\n","    checkpoint = torch.load(f\"./{MODEL_PATH}.pt\")\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","\n","for epoch in tqdm(range(N_EPOCHS), desc=\"Total Training Done:\", leave=False):\n","        # TRAINING\n","        train_loss = 0.0\n","        count = 0\n","        model.train()\n","        t_loader = iter(train_loader)\n","        t_correct, t_total = 0, 0\n","        for batch in tqdm(range(len(train_loader)), desc=f\"Epoch {epoch + 1}\", position=0):\n","            x, y = next(t_loader)\n","            x, y = x.to(DEVICE), y.to(DEVICE)\n","            y_hat = model(x)\n","            loss = criterion(y_hat, y) #/ len(x)\n","\n","            train_loss += loss.detach().cpu().item()\n","            \n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            \n","            t_correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n","            t_total += len(x)\n","            \n","            count+=1\n","            #if count>3000:\n","            #    break\n","        train_acc = t_correct / t_total * 100\n","        \n","        # TODO: LR DECAY\n","        \n","        # VALIDATION\n","        val_loss = 0.0\n","        v_correct, v_total = 0, 0\n","        model.eval()\n","        with torch.no_grad():\n","            v_loader = iter(val_loader)\n","            for batch in tqdm(range(len(val_loader)), desc=\"Validating\", position=0):\n","                x, y = next(v_loader)\n","                x, y = x.to(DEVICE), y.to(DEVICE)\n","                y_hat = model(x)\n","                loss = criterion(y_hat, y) #/ len(x)\n","                val_loss += loss.detach().cpu().item()\n","\n","                v_correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n","                v_total += len(x)\n","            \n","            val_acc = v_correct / v_total * 100\n","            if(best_val_acc < val_acc):\n","                best_model = copy.deepcopy(model)\n","                \n","                best_val_acc = val_acc\n","                print(f\"Best accuracy: {best_val_acc}\")\n","                torch.save({\n","                    'epoch': epoch,\n","                    'model_state_dict': best_model.state_dict(),\n","                    'optimizer_state_dict': optimizer.state_dict()\n","                }, f\"./{MODEL_PATH}.pt\")\n","                \n","                best_model = best_model.cpu()\n","                torch.cuda.empty_cache()\n","                \n","        print(f\"Epoch {epoch + 1}/{N_EPOCHS} train loss: {train_loss:.2f}, train acc {train_acc:.2f} val loss: {val_loss:.2f}\")\n","        print(f\"Val accuracy: {val_acc:.2f}%\")\n","        \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["del best_model, model, x, y, y_hat, loss, v_correct\n","gc.collect()\n","torch.cuda.empty_cache()\n","!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# val_acc, val_loss, train_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# import gc\n","# # del model\n","# del best_model\n","# gc.collect()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Test loop on latest model\n","# torch.cuda.empty_cache()\n","correct, total = 0, 0\n","test_loss = 0.0\n","model.eval()\n","with torch.no_grad():\n","    for batch in tqdm(test_loader, desc=\"Testing\", position=0, leave=True):\n","        x, y = batch\n","        x, y = x.to(DEVICE), y.to(DEVICE)\n","        y_hat = model(x)\n","        loss = criterion(y_hat, y) # / len(x)\n","        test_loss += loss.detach().cpu().item()\n","\n","        correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n","        total += len(x)\n","\n","        if total > 1000:\n","            break\n","    \n","print(\"Latest Model\")\n","print(f\"Test loss: {test_loss:.2f}\")\n","print(f\"Test accuracy: {correct / total * 100:.2f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Test loop on best model\n","# torch.cuda.empty_cache()\n","# test_model = ConvAcTransformer(d_model=512, attention_heads=4, num_layers=4, num_classes=101, feature_extractor_name='efficientnet_v2_s')\n","# test_model = test_model.to(device=DEVICE)\n","# MODEL_PATH = 'best_model_ucf101_x128'\n","\n","# criterion = nn.CrossEntropyLoss()\n","# checkpoint = torch.load(f\"./{MODEL_PATH}.pt\")\n","# test_model.load_state_dict(checkpoint['model_state_dict'])\n","\n","correct, total = 0, 0\n","test_loss = 0.0\n","best_model.eval()\n","for batch in tqdm(test_loader, desc=\"Testing\", position=0, leave=True):\n","    x, y = batch\n","    x, y = x.to(DEVICE), y.to(DEVICE)\n","    y_hat = best_model(x)\n","    loss = criterion(y_hat, y) # / len(x)\n","    test_loss += loss.detach().cpu().item()\n","\n","    correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n","    total += len(x)\n","\n","    if total > 17:\n","        break\n","    \n","print(\"Best Model\")\n","print(f\"Test loss: {test_loss:.2f}\")\n","print(f\"Test accuracy: {correct / total * 100:.2f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.13 ('hlcv')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"vscode":{"interpreter":{"hash":"48c45440f7e33ef530fe41f3ad259169edbefe38aafe97a40d9b60ea4ad6d240"}}},"nbformat":4,"nbformat_minor":4}
