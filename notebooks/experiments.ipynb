{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Training Notebook"]},{"cell_type":"markdown","metadata":{},"source":["we first need to build torchvision from source as it allows us to use ffmpeg and video_reader backend"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import distutils\n","distutils.spawn.find_executable('ffmpeg')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!apt install libavformat-dev libavcodec-dev libavdevice-dev libavutil-dev libswscale-dev libavresample-dev libavfilter-dev -y\n","!pip uninstall torchvision -y\n","!rm -r vision/\n","!git clone https://github.com/pytorch/vision.git\n","%cd vision\n","!python setup.py install\n","%cd /kaggle/working"]},{"cell_type":"markdown","metadata":{},"source":["## Restart but not reset"]},{"cell_type":"code","execution_count":78,"metadata":{"execution":{"iopub.execute_input":"2022-07-12T15:27:31.958999Z","iopub.status.busy":"2022-07-12T15:27:31.957898Z","iopub.status.idle":"2022-07-12T15:27:40.634180Z","shell.execute_reply":"2022-07-12T15:27:40.632997Z","shell.execute_reply.started":"2022-07-12T15:27:31.958953Z"},"trusted":true},"outputs":[],"source":["!pip show torchvision"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-07-12T15:27:40.637217Z","iopub.status.busy":"2022-07-12T15:27:40.636790Z","iopub.status.idle":"2022-07-12T15:27:40.695758Z","shell.execute_reply":"2022-07-12T15:27:40.694728Z","shell.execute_reply.started":"2022-07-12T15:27:40.637172Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/hamzamughal/anaconda3/envs/hlcv2/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import torchvision\n","import torch\n","from torch import nn, Tensor\n","from torchvision import models\n","import math\n","import gc\n","import random\n","\n","\n","random.seed(0)\n","torch.manual_seed(0)\n","DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') \n","torch.cuda.empty_cache()\n","# torchvision.set_video_backend('video_reader')"]},{"cell_type":"markdown","metadata":{},"source":["## Modelling Pipeline"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-07-12T16:50:14.361859Z","iopub.status.busy":"2022-07-12T16:50:14.361492Z","iopub.status.idle":"2022-07-12T16:50:14.992212Z","shell.execute_reply":"2022-07-12T16:50:14.991040Z","shell.execute_reply.started":"2022-07-12T16:50:14.361825Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([2, 10])\n","done\n"]},{"data":{"text/plain":["36"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["from torchvision.models.vision_transformer import Encoder\n","\n","class PatchEmbedding(nn.Module):\n","    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 15):\n","        super(PatchEmbedding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model), requires_grad=True)\n","        \n","        frames = max_len + 1\n","        position = torch.arange(frames).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n","        pe = torch.zeros(1, frames, d_model)\n","        pe[0, :, 0::2] = torch.sin(position * div_term)\n","        pe[0, :, 1::2] = torch.cos(position * div_term)\n","        self.register_buffer('pe', pe)\n","        \n","        # self.pe = nn.Parameter(torch.randn((1, frames, d_model)))\n","        # self.temp = nn.Parameter(torch.randn(1,max_len,d_model), requires_grad=True)\n","\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        \"\"\"\n","        Args:\n","            x: Tensor, shape [batch_size, n_frames, embedding_dim]\n","        \"\"\"\n","        \n","        b, _, _ = x.shape\n","        \n","        # temp = self.temp.expand(b, -1, -1)\n","        # x = x * temp\n","        \n","        cls_tokens = self.cls_token.expand(b, -1, -1)\n","        x = torch.cat([cls_tokens, x], dim=1)\n","        \n","        x = x + self.pe # [:, :x.size(1), :]\n","        return self.dropout(x)\n","\n","class FeatureExtractor(nn.Module):\n","    def __init__(self, d_model: int, model_name: str, model_weights: str = 'DEFAULT'):\n","        super(FeatureExtractor, self).__init__()\n","        # assert model_name in ['efficientnet_v2_s', 'efficientnet_v2_m', 'efficientnet_v2_l', 'resnet18', 'resnet50', 'inception_v3', 'wide_resnet50_2', 'wide_resnet101_2']\n","        self.model = getattr(models, model_name)(pretrained=True)\n","        \n","        if model_name in ['efficientnet_v2_s', 'efficientnet_v2_m', 'efficientnet_v2_l']:\n","            self.model.classifier = nn.Linear(in_features=self.model.classifier[1].in_features, out_features = d_model)\n","        else:\n","            self.model.fc = nn.Linear(in_features=self.model.fc.in_features ,out_features=d_model)\n","            \n","        \n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        \"\"\"\n","        Args:\n","            x: Tensor, shape [batch_size, n_frames, channels, height, width]\n","        \"\"\"\n","        b, f, _, _, _ = x.shape\n","        x = x.view(b*f, *x.size()[2:])\n","        x = self.model(x)\n","        x = x.view(b, f, *x.size()[1:])\n","        \n","        return x\n","\n","\n","class ConvAcTransformer(nn.Module):\n","    def __init__(self, d_model: int,\n","                 attention_heads: int,\n","                 num_layers: int,\n","                 num_classes: int,\n","                 num_frames: int,\n","                 feature_extractor_name: str):\n","        super(ConvAcTransformer, self).__init__()\n","        self.d_model = d_model\n","        self.feature_extractor_name = feature_extractor_name\n","        self.attention_heads = attention_heads\n","        self.num_classes = num_classes\n","        self.num_layers = num_layers\n","        self.num_frames = num_frames\n","\n","        \n","        self.feature_extract = FeatureExtractor(self.d_model, self.feature_extractor_name, model_weights='DEFAULT')\n","        self.patch_embed = PatchEmbedding(self.d_model)\n","        \n","        self.transformer_encoder = Encoder(seq_length=self.num_frames+1,\n","                                           num_layers=self.num_layers,\n","                                           num_heads=self.attention_heads,\n","                                           hidden_dim=self.d_model, \n","                                           mlp_dim=self.d_model,\n","                                           dropout=0.1, attention_dropout=0.1)\n","\n","        self.classification_head = nn.Linear(self.d_model, self.num_classes)\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        \"\"\"\n","        Args:\n","            x: Tensor, shape [batch_size, n_frames, channels, height, width]\n","        \"\"\"\n","        # extract features from all frames\n","        x = self.feature_extract(x)\n","\n","        # apply patch embedding from ViT\n","        x = self.patch_embed(x)\n","\n","        # ViT encoder\n","        x = self.transformer_encoder(x)\n","\n","        # select first token/classifier token\n","        x = x[:, 0, :]\n","\n","        # classification head\n","        x = self.classification_head(x)\n","\n","        return x\n","\n","\n","# test\n","test_tensor = torch.randn(2, 15, 3, 128, 128, dtype=torch.float32).to(DEVICE)\n","model = ConvAcTransformer(d_model=256, attention_heads=4, num_frames=15, num_layers=4, num_classes=10, feature_extractor_name='resnet18')\n","model = model.to(device=DEVICE)\n","out = model(test_tensor)\n","print(out.size())\n","diff = out.mean().backward()\n","print(\"done\")\n","del test_tensor,out, diff\n","gc.collect()"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-07-12T14:56:38.265692Z","iopub.status.busy":"2022-07-12T14:56:38.265071Z","iopub.status.idle":"2022-07-12T14:56:38.270053Z","shell.execute_reply":"2022-07-12T14:56:38.268808Z","shell.execute_reply.started":"2022-07-12T14:56:38.265654Z"},"trusted":true},"outputs":[],"source":["# list(model.feature_extract.named_parameters())[0][1].grad#.size() #[0,14]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# a = torch.randn(16,15,3,128,128)\n","# a[:, 0].size()"]},{"cell_type":"markdown","metadata":{},"source":["## Data Pipeline"]},{"cell_type":"markdown","metadata":{},"source":["#### Data download using next cell: (comment if it's already there)"]},{"cell_type":"markdown","metadata":{},"source":["UCF-101 Download:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# !apt install unrar \n","# !wget https://www.crcv.ucf.edu/data/UCF101/UCF101.rar --no-check-certificate\n","# !wget https://www.crcv.ucf.edu/data/UCF101/UCF101TrainTestSplits-RecognitionTask.zip --no-check-certificate\n","# !unrar  x UCF101.rar -idq\n","# !unzip UCF101TrainTestSplits-RecognitionTask.zip\n","# ! rm UCF101.rar UCF101TrainTestSplits-RecognitionTask.zip"]},{"cell_type":"markdown","metadata":{},"source":["UCF-50 Download:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-07-12T13:09:40.574789Z","iopub.status.busy":"2022-07-12T13:09:40.574071Z","iopub.status.idle":"2022-07-12T13:14:55.713545Z","shell.execute_reply":"2022-07-12T13:14:55.712270Z","shell.execute_reply.started":"2022-07-12T13:09:40.574748Z"},"trusted":true},"outputs":[],"source":["# !apt install unrar \n","# !wget https://www.crcv.ucf.edu/data/UCF50.rar --no-check-certificate\n","# !unrar x UCF50.rar -idq\n","# !mkdir ./ucf50TrainTestlist\n","# !wget -P ./ucf50TrainTestlist https://github.com/temur-kh/video-classification-cv/raw/master/data/classInd.txt\n","# !wget -O ./ucf50TrainTestlist/testlist01.txt https://github.com/temur-kh/video-classification-cv/raw/master/data/testlist.txt\n","# !wget -O ./ucf50TrainTestlist/trainlist01.txt https://github.com/temur-kh/video-classification-cv/raw/master/data/trainlist.txt\n","# ! rm UCF50.rar "]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-07-12T15:39:23.418267Z","iopub.status.busy":"2022-07-12T15:39:23.417758Z","iopub.status.idle":"2022-07-12T15:39:23.451345Z","shell.execute_reply":"2022-07-12T15:39:23.450297Z","shell.execute_reply.started":"2022-07-12T15:39:23.418221Z"},"trusted":true},"outputs":[],"source":["# class_ind = [\"BaseballPitch\", \"Basketball\", \"BenchPress\", \"Biking\", \"Billiards\", \"BreastStroke\", \"CleanAndJerk\", \"Diving\", \"Drumming\", \"Fencing\"]\n","# files = [\"testlist01.txt\", \"trainlist01.txt\", \"classInd.txt\"]\n","# for file in files:\n","#     with open(f\"../datasets/ucf10TrainTestlist/{file}\", \"r+\") as f:\n","#         d = f.readlines()\n","#         f.seek(0)\n","#         for i in d:\n","#             if any(substring in i for substring in class_ind):\n","#                 f.write(i)\n","#         f.truncate()\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-07-12T15:39:46.063989Z","iopub.status.busy":"2022-07-12T15:39:46.063621Z","iopub.status.idle":"2022-07-12T15:39:46.456918Z","shell.execute_reply":"2022-07-12T15:39:46.455887Z","shell.execute_reply.started":"2022-07-12T15:39:46.063956Z"},"trusted":true},"outputs":[],"source":["# import os, shutil\n","# os.listdir('../datasets/UCF10')\n","# for i in os.listdir(\"../datasets/UCF10\"):\n","#     if i not in class_ind:\n","#         shutil.rmtree(f'../datasets/UCF10/{i}')\n","        \n"]},{"cell_type":"code","execution_count":80,"metadata":{"execution":{"iopub.execute_input":"2022-07-12T15:38:18.798156Z","iopub.status.busy":"2022-07-12T15:38:18.797391Z","iopub.status.idle":"2022-07-12T15:38:39.641418Z","shell.execute_reply":"2022-07-12T15:38:39.640267Z","shell.execute_reply.started":"2022-07-12T15:38:18.798117Z"},"trusted":true},"outputs":[],"source":["# !pip install pytorchvideo"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-07-12T16:50:27.475486Z","iopub.status.busy":"2022-07-12T16:50:27.475138Z","iopub.status.idle":"2022-07-12T16:51:04.582219Z","shell.execute_reply":"2022-07-12T16:51:04.580969Z","shell.execute_reply.started":"2022-07-12T16:50:27.475456Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 87/87 [00:58<00:00,  1.48it/s]\n"]}],"source":["from torchvision.datasets import UCF101\n","from torchvision import transforms\n","\n","from torchvision.transforms import Compose, Lambda\n","# from torchvision.transforms._transforms_video import (\n","#     CenterCropVideo,\n","#     NormalizeVideo,\n","# )\n","# from pytorchvideo.transforms import (\n","#     ApplyTransformToKey,\n","#     ShortSideScale,\n","#     UniformTemporalSubsample\n","# )\n","side_size = 128\n","mean = [0.485, 0.456, 0.406] \n","std = [0.229, 0.224, 0.225]\n","crop_size = 128\n","num_frames = 8\n","sampling_rate = 8\n","frames_per_second = 30\n","\n","\n","tfs = transforms.Compose([\n","    # transforms.Lambda(lambda x: x.permute(0, 3, 1, 2)),\n","    transforms.Lambda(lambda x: x[::3]), # skip second frame\n","    # transforms.RandomHorizontalFlip(p=0.5),\n","    # transforms.RandomResizedCrop(256, scale=(0.8, 1.0)),\n","    # transforms.CenterCrop(60),\n","    # transforms.RandomRotation(degrees=10, interpolation=transforms.InterpolationMode.BILINEAR),\n","    # transforms.GrayScale(),\n","    # transforms.GaussianBlur(kernel_size=3),\n","    # transforms.ColorJitter(brightness=.2, hue=.1),\n","    # transforms.RandomPerspective(distortion_scale=0.1),\n","    # transforms.AugMix(),\n","    # transforms.RandAugment(),\n","    Lambda(lambda x: x.permute(3, 0, 1, 2)), # THWC -> CTHW\n","    Lambda(lambda x: x/255.0),\n","    # NormalizeVideo(mean, std),\n","    #ShortSideScale(size=side_size),\n","    # CenterCropVideo(crop_size=(crop_size, crop_size)),\n","    Lambda(lambda x: x.permute(1, 0, 2, 3)), # CTHW -> TCHW\n","    transforms.Resize((side_size, side_size)),\n","\n","    # transforms.Lambda(lambda x: x / 255.),\n","    transforms.Lambda(lambda x: x.float()),\n","])\n","\n","val_tfs = transforms.Compose([\n","        transforms.Lambda(lambda x: x.permute(0, 3, 1, 2)),\n","        transforms.Lambda(lambda x: x[::60//20]), # skip second frame\n","        transforms.Resize((128, 128)),\n","        transforms.Lambda(lambda x: x.float()),\n","        # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n","        # transforms.Lambda(lambda x: x / 255.),s\n","        \n","    ])\n","\n","# train_dataset = UCF101(root = '../datasets/UCF10/', annotation_path = '../datasets/ucf10TrainTestlist/', transform=transforms, train=True, frames_per_clip=60) # ,_video_width=128, _video_height=128)\n","valtest_dataset = UCF101(root = '../datasets/UCF10/', annotation_path = '../datasets/ucf10TrainTestlist/', transform=val_tfs, train=False, frames_per_clip=60)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-07-12T15:55:47.841507Z","iopub.status.busy":"2022-07-12T15:55:47.841100Z","iopub.status.idle":"2022-07-12T15:55:47.847325Z","shell.execute_reply":"2022-07-12T15:55:47.846076Z","shell.execute_reply.started":"2022-07-12T15:55:47.841470Z"},"trusted":true},"outputs":[],"source":["# train_dataset[0]"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-07-12T16:51:08.136023Z","iopub.status.busy":"2022-07-12T16:51:08.135634Z","iopub.status.idle":"2022-07-12T16:51:08.143331Z","shell.execute_reply":"2022-07-12T16:51:08.142336Z","shell.execute_reply.started":"2022-07-12T16:51:08.135974Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'train_dataset' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m/home/hamzamughal/Desktop/UdS MS/SoSe22/HLCV/Project/Code/Conv-AcT-pytorch/notebooks/experiments.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hamzamughal/Desktop/UdS%20MS/SoSe22/HLCV/Project/Code/Conv-AcT-pytorch/notebooks/experiments.ipynb#ch0000022?line=0'>1</a>\u001b[0m \u001b[39m# del model, x, y, loss\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hamzamughal/Desktop/UdS%20MS/SoSe22/HLCV/Project/Code/Conv-AcT-pytorch/notebooks/experiments.ipynb#ch0000022?line=1'>2</a>\u001b[0m \u001b[39m# gc.collect()\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hamzamughal/Desktop/UdS%20MS/SoSe22/HLCV/Project/Code/Conv-AcT-pytorch/notebooks/experiments.ipynb#ch0000022?line=2'>3</a>\u001b[0m \u001b[39mlen\u001b[39m(train_dataset\u001b[39m.\u001b[39mclasses)\n","\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"]}],"source":["# del model, x, y, loss\n","# gc.collect()\n","len(train_dataset.classes)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-07-12T16:51:10.049746Z","iopub.status.busy":"2022-07-12T16:51:10.049077Z","iopub.status.idle":"2022-07-12T16:51:10.063150Z","shell.execute_reply":"2022-07-12T16:51:10.062006Z","shell.execute_reply.started":"2022-07-12T16:51:10.049710Z"},"trusted":true},"outputs":[],"source":["\n","indices = random.sample(list(range(len(train_dataset))), 700)\n","\n","# Warp into Subsets\n","train_subset = torch.utils.data.Subset(train_dataset, indices[:-200])\n","test_subset = torch.utils.data.Subset(train_dataset, indices[-100:])\n","val_subset = torch.utils.data.Subset(train_dataset, indices[-200:-100])"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-07-12T16:51:12.251944Z","iopub.status.busy":"2022-07-12T16:51:12.251577Z","iopub.status.idle":"2022-07-12T16:51:12.262677Z","shell.execute_reply":"2022-07-12T16:51:12.261168Z","shell.execute_reply.started":"2022-07-12T16:51:12.251912Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Total number of train samples: 500\n","Total number of test samples: 100\n","Total number of val samples: 100\n","Total number of (train) batches: 32\n","Total number of (test) batches: 7\n","Total number of (val) batches: 7\n"]}],"source":["def custom_collate(batch):\n","    # skip audio data\n","    filtered_batch = []\n","    for video, _, label in batch:\n","        filtered_batch.append((video, label))\n","    return torch.utils.data.dataloader.default_collate(filtered_batch)\n","\n","BATCH_SIZE = 16\n","train_loader = torch.utils.data.DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True,\n","                                           num_workers=2, pin_memory=True,\n","                                           collate_fn=custom_collate)\n","val_loader = torch.utils.data.DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=True,\n","                                           num_workers=2, pin_memory=True,\n","                                           collate_fn=custom_collate)\n","test_loader = torch.utils.data.DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True,\n","                                           num_workers=2,\n","                                           collate_fn=custom_collate)\n","\n","print(f\"Total number of train samples: {len(train_subset)}\")\n","print(f\"Total number of test samples: {len(test_subset)}\")\n","print(f\"Total number of val samples: {len(val_subset)}\")\n","print(f\"Total number of (train) batches: {len(train_loader)}\")\n","print(f\"Total number of (test) batches: {len(test_loader)}\")\n","print(f\"Total number of (val) batches: {len(val_loader)}\")"]},{"cell_type":"markdown","metadata":{},"source":["### Visualization"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-07-12T16:51:14.768631Z","iopub.status.busy":"2022-07-12T16:51:14.767947Z","iopub.status.idle":"2022-07-12T16:51:16.959253Z","shell.execute_reply":"2022-07-12T16:51:16.958063Z","shell.execute_reply.started":"2022-07-12T16:51:14.768591Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/hamzamughal/anaconda3/envs/hlcv2/lib/python3.8/site-packages/torchvision/io/video.py:162: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n","  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n","/home/hamzamughal/anaconda3/envs/hlcv2/lib/python3.8/site-packages/torchvision/io/video.py:162: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n","  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n"]},{"name":"stdout","output_type":"stream","text":["Dataset batch shape torch.Size([16, 15, 3, 128, 128]) torch.float32 tensor([1, 3, 2, 4, 8, 9, 5, 1, 3, 4, 0, 6, 7, 1, 4, 6]) torch.int64\n","Example: Basketball\n"]},{"data":{"text/html":["<video controls  width=\"256\"  height=\"256\">\n"," <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\">\n"," Your browser does not support the video tag.\n"," </video>"],"text/plain":["<IPython.core.display.Video object>"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# !pip install imageio-ffmpeg\n","import imageio\n","from IPython.display import Video\n","import numpy as np\n","\n","for i, (fm,lb) in enumerate(train_loader):\n","    print(\"Dataset batch shape\", fm.size(), fm.dtype, lb,  lb.dtype )\n","    # break\n","    video = fm[0].permute(0, 2, 3, 1).numpy() \n","    # print(video.min(), video.max(), video.mean(), video.std())\n","    video = (video * 255.0).astype(np.uint8)\n","    label = train_dataset.classes[lb[0]]\n","    \n","    print(\"Example:\", label)\n","    imageio.mimwrite('./test.mp4', video, fps=30)\n","    break\n","\n","Video('./test.mp4', width=256, height=256, embed=True)"]},{"cell_type":"markdown","metadata":{},"source":["## Training Pipeline"]},{"cell_type":"code","execution_count":144,"metadata":{"execution":{"iopub.execute_input":"2022-07-12T16:51:22.659972Z","iopub.status.busy":"2022-07-12T16:51:22.659549Z","iopub.status.idle":"2022-07-12T16:51:23.474747Z","shell.execute_reply":"2022-07-12T16:51:23.473485Z","shell.execute_reply.started":"2022-07-12T16:51:22.659933Z"},"trusted":true},"outputs":[],"source":["torch.cuda.empty_cache()\n","!nvidia-smi"]},{"cell_type":"code","execution_count":145,"metadata":{"execution":{"iopub.execute_input":"2022-07-12T16:51:34.570998Z","iopub.status.busy":"2022-07-12T16:51:34.569969Z","iopub.status.idle":"2022-07-12T16:51:34.596780Z","shell.execute_reply":"2022-07-12T16:51:34.595570Z","shell.execute_reply.started":"2022-07-12T16:51:34.570955Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from matplotlib.lines import Line2D\n","from IPython import display\n","import time\n","%matplotlib inline\n","\n","def plot_grad_flow(named_parameters):\n","    '''Plots the gradients flowing through different layers in the net during training.\n","    Can be used for checking for possible gradient vanishing / exploding problems.\n","    \n","    Usage: Plug this function in Trainer class after loss.backwards() as \n","    \"plot_grad_flow(self.model.named_parameters())\" to visualize the gradient flow'''\n","    ave_grads = []\n","    max_grads= []\n","    layers = []\n","    for n, p in named_parameters:\n","#         print(n)\n","        if(p.requires_grad) and (\"bias\" not in n):\n","            layers.append(n)\n","#             print(n)\n","            ave_grads.append(p.grad.abs().mean().detach().cpu())\n","            max_grads.append(p.grad.abs().max().detach().cpu())\n","            \n","#         if len(max_grads)> 10:\n","#             break\n","    max_grads = max_grads[-60:]\n","    ave_grads = ave_grads[-60:]\n","    layers = layers[-60:]\n","    plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color=\"c\")\n","    plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color=\"b\")\n","    plt.hlines(0, 0, len(ave_grads)+1, lw=2, color=\"k\" )\n","    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n","    plt.xlim(left=0, right=len(ave_grads))\n","    plt.ylim(bottom = -0.001, top=0.02) # zoom in on the lower gradient regions\n","    plt.xlabel(\"Layers\")\n","    plt.ylabel(\"average gradient\")\n","    plt.title(\"Gradient flow\")\n","    plt.grid(True)\n","    plt.legend([Line2D([0], [0], color=\"c\", lw=4),\n","                Line2D([0], [0], color=\"b\", lw=4),\n","                Line2D([0], [0], color=\"k\", lw=4)], ['max-gradient', 'mean-gradient', 'zero-gradient'])\n"]},{"cell_type":"code","execution_count":146,"metadata":{"execution":{"iopub.execute_input":"2022-07-12T16:51:37.000145Z","iopub.status.busy":"2022-07-12T16:51:36.999770Z","iopub.status.idle":"2022-07-12T16:51:37.005058Z","shell.execute_reply":"2022-07-12T16:51:37.003798Z","shell.execute_reply.started":"2022-07-12T16:51:37.000113Z"},"trusted":true},"outputs":[],"source":["# list(model.named_parameters())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-07-12T16:57:34.998864Z","iopub.status.busy":"2022-07-12T16:57:34.998494Z"},"trusted":true},"outputs":[],"source":["from torch import optim\n","from tqdm import tqdm\n","import copy\n","\n","# Defining model and training options\n","model = model.to(device=DEVICE)\n","\n","N_EPOCHS = 200\n","LR = 0.01\n","MODEL_PATH = 'best_model_ucf50_x128_sub_30'\n","best_val_acc = 0\n","best_model = None\n","pretrain = False\n","\n","# Training loop\n","optimizer = optim.Adam(model.parameters(), lr=LR) #, weight_decay=0.1)\n","criterion = nn.CrossEntropyLoss(reduction='mean')\n","scheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.005, max_lr=LR, cycle_momentum=False, verbose=True)\n","\n","if pretrain:\n","    checkpoint = torch.load(f\"./{MODEL_PATH}.pt\")\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","\n","for epoch in tqdm(range(N_EPOCHS), desc=\"Total Training Done:\", leave=False):\n","        # TRAINING\n","        train_loss = 0.0\n","        count = 0\n","        model.train()\n","        t_loader = iter(train_loader)\n","        t_correct, t_total = 0, 0\n","        # plt.figure()\n","        for batch in tqdm(range(len(train_loader)), desc=f\"Epoch {epoch + 1}\", position=0):\n","            x, y = next(t_loader)\n","            x, y = x.to(DEVICE), y.to(DEVICE)\n","            y_hat = model(x)\n","            loss = criterion(y_hat, y) #/ len(x)\n","\n","            train_loss += loss.detach().cpu().item()\n","            #print(loss.detach().cpu().item())\n","            optimizer.zero_grad()\n","            loss.backward()  \n","            optimizer.step()\n","            \n","            t_correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n","            t_total += len(x)\n","            \n","            count+=1\n","\n","            #if count>3000:\n","            #    break\n","        train_acc = t_correct / t_total * 100\n","        \n","        scheduler.step()\n","        \n","        display.clear_output(wait=True)\n","        plt.clf()\n","        plt.figure(figsize=(20, 5))\n","        plot_grad_flow(model.named_parameters())\n","        display.display(plt.gcf())\n","        time.sleep(1)\n","        \n","        print(f\"Epoch {epoch + 1}/{N_EPOCHS} train loss: {train_loss/count:.2f}, train acc {train_acc:.2f}\")\n","        # TODO: LR DECAY\n","        \n","        # VALIDATION\n","        val_loss = 0.0\n","        v_correct, v_total = 0, 0\n","        vcount = 0\n","        model.eval()\n","        with torch.no_grad():\n","            v_loader = iter(val_loader)\n","            for batch in tqdm(range(len(val_loader)), desc=\"Validating\", position=0):\n","                x, y = next(v_loader)\n","                x, y = x.to(DEVICE), y.to(DEVICE)\n","                y_hat = model(x)\n","                loss = criterion(y_hat, y) #/ len(x)\n","                val_loss += loss.detach().cpu().item()\n","\n","                v_correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n","                v_total += len(x)\n","                \n","                vcount+=1\n","            \n","            val_acc = v_correct / v_total * 100\n","            if(best_val_acc < val_acc):\n","                best_model = copy.deepcopy(model)\n","                \n","                best_val_acc = val_acc\n","                print(f\"Best accuracy: {best_val_acc}\")\n","                torch.save({\n","                    'epoch': epoch,\n","                    'model_state_dict': best_model.state_dict(),\n","                    'optimizer_state_dict': optimizer.state_dict()\n","                }, f\"./{MODEL_PATH}.pt\")\n","                \n","                best_model = best_model.cpu()\n","                torch.cuda.empty_cache()\n","        \n","        print(f\"Epoch {epoch + 1}/{N_EPOCHS} val loss: {val_loss/vcount:.2f} Val accuracy: {val_acc:.2f}%\")\n","        \n","\n"]},{"cell_type":"code","execution_count":136,"metadata":{"execution":{"iopub.execute_input":"2022-07-12T16:47:56.211867Z","iopub.status.busy":"2022-07-12T16:47:56.211343Z","iopub.status.idle":"2022-07-12T16:47:56.220395Z","shell.execute_reply":"2022-07-12T16:47:56.219329Z","shell.execute_reply.started":"2022-07-12T16:47:56.211818Z"},"trusted":true},"outputs":[],"source":["best_val_acc"]},{"cell_type":"code","execution_count":137,"metadata":{"execution":{"iopub.execute_input":"2022-07-12T16:50:03.827497Z","iopub.status.busy":"2022-07-12T16:50:03.827091Z","iopub.status.idle":"2022-07-12T16:50:04.981695Z","shell.execute_reply":"2022-07-12T16:50:04.980335Z","shell.execute_reply.started":"2022-07-12T16:50:03.827467Z"},"trusted":true},"outputs":[],"source":["del best_model, model, x, y, y_hat, loss, v_correct\n","gc.collect()\n","torch.cuda.empty_cache()\n","!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# val_acc, val_loss, train_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# import gc\n","# # del model\n","# del best_model\n","# gc.collect()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Test loop on latest model\n","# torch.cuda.empty_cache()\n","correct, total = 0, 0\n","test_loss = 0.0\n","model.eval()\n","with torch.no_grad():\n","    for batch in tqdm(test_loader, desc=\"Testing\", position=0, leave=True):\n","        x, y = batch\n","        x, y = x.to(DEVICE), y.to(DEVICE)\n","        y_hat = model(x)\n","        loss = criterion(y_hat, y) # / len(x)\n","        test_loss += loss.detach().cpu().item()\n","\n","        correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n","        total += len(x)\n","\n","        if total > 1000:\n","            break\n","    \n","print(\"Latest Model\")\n","print(f\"Test loss: {test_loss:.2f}\")\n","print(f\"Test accuracy: {correct / total * 100:.2f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Test loop on best model\n","# torch.cuda.empty_cache()\n","# test_model = ConvAcTransformer(d_model=512, attention_heads=4, num_layers=4, num_classes=101, feature_extractor_name='efficientnet_v2_s')\n","# test_model = test_model.to(device=DEVICE)\n","# MODEL_PATH = 'best_model_ucf101_x128'\n","\n","# criterion = nn.CrossEntropyLoss()\n","# checkpoint = torch.load(f\"./{MODEL_PATH}.pt\")\n","# test_model.load_state_dict(checkpoint['model_state_dict'])\n","\n","correct, total = 0, 0\n","test_loss = 0.0\n","best_model.eval()\n","for batch in tqdm(test_loader, desc=\"Testing\", position=0, leave=True):\n","    x, y = batch\n","    x, y = x.to(DEVICE), y.to(DEVICE)\n","    y_hat = best_model(x)\n","    loss = criterion(y_hat, y) # / len(x)\n","    test_loss += loss.detach().cpu().item()\n","\n","    correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n","    total += len(x)\n","\n","    if total > 17:\n","        break\n","    \n","print(\"Best Model\")\n","print(f\"Test loss: {test_loss:.2f}\")\n","print(f\"Test accuracy: {correct / total * 100:.2f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.13 ('hlcv2')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"vscode":{"interpreter":{"hash":"09f02e43fdd2162b599f605989e2aba25f064ed46c7c007eae528ee947b6bb15"}}},"nbformat":4,"nbformat_minor":4}
