{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Training Notebook","metadata":{}},{"cell_type":"markdown","source":"we first need to build torchvision from source as it allows us to use ffmpeg and video_reader backend","metadata":{}},{"cell_type":"code","source":"import distutils\ndistutils.spawn.find_executable('ffmpeg')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!apt install libavformat-dev libavcodec-dev libavdevice-dev libavutil-dev libswscale-dev libavresample-dev libavfilter-dev -y\n!pip uninstall torchvision -y\n!rm -r vision/\n!git clone https://github.com/pytorch/vision.git\n%cd vision\n!python setup.py install\n%cd /kaggle/working","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Restart but not reset","metadata":{}},{"cell_type":"code","source":"!pip show torchvision","metadata":{"execution":{"iopub.status.busy":"2022-07-06T19:40:17.29367Z","iopub.execute_input":"2022-07-06T19:40:17.294708Z","iopub.status.idle":"2022-07-06T19:40:26.028041Z","shell.execute_reply.started":"2022-07-06T19:40:17.294554Z","shell.execute_reply":"2022-07-06T19:40:26.026857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision\nimport torch\nfrom torch import nn, Tensor\nfrom torchvision import models\nimport math\nimport gc\nimport random\n\n\nrandom.seed(0)\ntorch.manual_seed(0)\nDEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') \ntorch.cuda.empty_cache()\ntorchvision.set_video_backend('video_reader')","metadata":{"execution":{"iopub.status.busy":"2022-07-06T20:04:05.129311Z","iopub.execute_input":"2022-07-06T20:04:05.129690Z","iopub.status.idle":"2022-07-06T20:04:05.217633Z","shell.execute_reply.started":"2022-07-06T20:04:05.129636Z","shell.execute_reply":"2022-07-06T20:04:05.216533Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Modelling Pipeline","metadata":{}},{"cell_type":"code","source":"class PatchEmbedding(nn.Module):\n    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n        super(PatchEmbedding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n\n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe = torch.zeros(1, max_len, d_model)\n        pe[0, :, 0::2] = torch.sin(position * div_term)\n        pe[0, :, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"\n        Args:\n            x: Tensor, shape [batch_size, n_frames, embedding_dim]\n        \"\"\"\n        b, _, _ = x.shape\n        cls_tokens = self.cls_token.expand(b, -1, -1)\n        x = torch.cat([cls_tokens, x], dim=1)\n        x = x + self.pe[:, :x.size(1), :]\n        return self.dropout(x)\n\nclass FeatureExtractor(nn.Module):\n    def __init__(self, d_model: int, model_name: str, model_weights: str = 'DEFAULT'):\n        super(FeatureExtractor, self).__init__()\n        assert model_name in ['efficientnet_v2_s', 'efficientnet_v2_m', 'efficientnet_v2_l', 'inception_v3', 'wide_resnet50_2', 'wide_resnet101_2']\n        self.model = getattr(models, model_name)(weights=model_weights)\n        \n        if model_name in ['efficientnet_v2_s', 'efficientnet_v2_m', 'efficientnet_v2_l']:\n            self.model.classifier = nn.Linear(in_features=self.model.classifier[1].in_features, out_features = d_model)\n        else:\n            self.model.fc = nn.Linear(in_features=self.model.fc.in_features ,out_features=d_model)\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"\n        Args:\n            x: Tensor, shape [batch_size, n_frames, channels, height, width]\n        \"\"\"\n        b, f, _, _, _ = x.shape\n        x = x.view(b*f, *x.size()[2:])\n        x = self.model(x)\n        x = x.view(b, f, *x.size()[1:])\n\n        return x\n\n\nclass ConvAcTransformer(nn.Module):\n    def __init__(self, d_model: int,\n                 attention_heads: int,\n                 num_layers: int,\n                 num_classes: int,\n                 feature_extractor_name: str):\n        super(ConvAcTransformer, self).__init__()\n        self.d_model = d_model\n        self.feature_extractor_name = feature_extractor_name\n        self.attention_heads = attention_heads\n        self.num_classes = num_classes\n        self.num_layers = num_layers\n\n        \n        self.feature_extract = FeatureExtractor(self.d_model, self.feature_extractor_name, model_weights=None)\n        self.patch_embed = PatchEmbedding(self.d_model)\n\n        transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=self.d_model,\n                                                                nhead=self.attention_heads,\n                                                                norm_first=True,\n                                                                activation='gelu')\n        self.transformer_encoder = nn.TransformerEncoder(transformer_encoder_layer,\n                                                         self.num_layers,\n                                                         norm=nn.LayerNorm(self.d_model))\n\n        self.classification_head = nn.Linear(self.d_model, self.num_classes)\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"\n        Args:\n            x: Tensor, shape [batch_size, n_frames, channels, height, width]\n        \"\"\"\n        # extract features from all frames\n        x = self.feature_extract(x)\n\n        # apply patch embedding from ViT\n        x = self.patch_embed(x)\n\n        # ViT encoder\n        x = self.transformer_encoder(x)\n\n        # select first token/classifier token\n        x = x[:, 0, :]\n\n        # classification head\n        x = self.classification_head(x)\n\n        return x\n\n\n# test\ntest_tensor = torch.randn(16, 15, 3, 128, 128, dtype=torch.float32).to(DEVICE)\nmodel = ConvAcTransformer(d_model=256, attention_heads=2, num_layers=2, num_classes=101, feature_extractor_name='efficientnet_v2_s')\nmodel = model.to(device=DEVICE)\nout = model(test_tensor)\nprint(out.size())\ndiff = out.mean().backward()\nprint(\"done\")\ndel test_tensor, out, diff\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-07-06T20:41:43.699346Z","iopub.execute_input":"2022-07-06T20:41:43.699832Z","iopub.status.idle":"2022-07-06T20:41:45.586941Z","shell.execute_reply.started":"2022-07-06T20:41:43.699789Z","shell.execute_reply":"2022-07-06T20:41:45.586003Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"## Data Pipeline","metadata":{}},{"cell_type":"markdown","source":"#### Data download using next cell: (comment if it's already there)","metadata":{}},{"cell_type":"code","source":"!apt install unrar \n!wget https://www.crcv.ucf.edu/data/UCF101/UCF101.rar --no-check-certificate\n!wget https://www.crcv.ucf.edu/data/UCF101/UCF101TrainTestSplits-RecognitionTask.zip --no-check-certificate\n!unrar  x UCF101.rar -idq\n!unzip UCF101TrainTestSplits-RecognitionTask.zip\n! rm UCF101.rar UCF101TrainTestSplits-RecognitionTask.zip","metadata":{"execution":{"iopub.status.busy":"2022-07-06T19:41:43.694501Z","iopub.execute_input":"2022-07-06T19:41:43.695153Z","iopub.status.idle":"2022-07-06T19:48:14.999604Z","shell.execute_reply.started":"2022-07-06T19:41:43.69511Z","shell.execute_reply":"2022-07-06T19:48:14.998273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision.datasets import UCF101\nfrom torchvision import transforms\n\ntransforms = transforms.Compose([\n    transforms.Lambda(lambda x: x.permute(0, 3, 1, 2)),\n    transforms.Lambda(lambda x: x[::2]), # skip second frame\n    transforms.RandomHorizontalFlip(p=0.5),\n    # transforms.RandomResizedCrop(256, scale=(0.8, 1.0)),\n    # transforms.CenterCrop(60),\n    # transforms.RandomRotation(degrees=10, interpolation=transforms.InterpolationMode.BILINEAR),\n    # transforms.GrayScale(),\n    # transforms.GaussianBlur(kernel_size=3),\n    # transforms.ColorJitter(brightness=.2, hue=.1),\n    # transforms.RandomPerspective(distortion_scale=0.1),\n    # transforms.AugMix(),\n    # transforms.RandAugment(),\n\n    transforms.Lambda(lambda x: x / 255.),\n    transforms.Lambda(lambda x: x.float()),\n])\n\ntrain_dataset = UCF101(root = './UCF-101/', annotation_path = './ucfTrainTestlist/', transform=transforms ,_video_width=128, _video_height=128, train=True, frames_per_clip=30)\n# valtest_dataset = UCF101(root = './UCF-101/', annotation_path = './ucfTrainTestlist/', transform=transforms ,_video_width=128, _video_height=128, train=False, frames_per_clip=30)","metadata":{"execution":{"iopub.status.busy":"2022-07-06T20:04:13.309395Z","iopub.execute_input":"2022-07-06T20:04:13.309767Z","iopub.status.idle":"2022-07-06T20:11:56.164190Z","shell.execute_reply.started":"2022-07-06T20:04:13.309736Z","shell.execute_reply":"2022-07-06T20:11:56.163081Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# del model, x, y, loss\n# gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-07-06T20:11:56.930674Z","iopub.execute_input":"2022-07-06T20:11:56.931686Z","iopub.status.idle":"2022-07-06T20:11:56.936525Z","shell.execute_reply.started":"2022-07-06T20:11:56.931629Z","shell.execute_reply":"2022-07-06T20:11:56.935621Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"\nindices = random.sample(list(range(len(train_dataset))), len(train_dataset)//30)\n\n# Warp into Subsets\ntrain_subset = torch.utils.data.Subset(train_dataset, indices[:-2000])\ntest_subset = torch.utils.data.Subset(train_dataset, indices[-1000:])\nval_subset = torch.utils.data.Subset(train_dataset, indices[-2000:-1000])","metadata":{"execution":{"iopub.status.busy":"2022-07-06T20:39:47.280125Z","iopub.execute_input":"2022-07-06T20:39:47.280715Z","iopub.status.idle":"2022-07-06T20:39:47.381530Z","shell.execute_reply.started":"2022-07-06T20:39:47.280676Z","shell.execute_reply":"2022-07-06T20:39:47.380533Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"def custom_collate(batch):\n    # skip audio data\n    filtered_batch = []\n    for video, _, label in batch:\n        filtered_batch.append((video, label))\n    return torch.utils.data.dataloader.default_collate(filtered_batch)\n\nBATCH_SIZE = 16\ntrain_loader = torch.utils.data.DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True,\n                                           num_workers=2, pin_memory=True,\n                                           collate_fn=custom_collate)\nval_loader = torch.utils.data.DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=True,\n                                           num_workers=2, pin_memory=True,\n                                           collate_fn=custom_collate)\ntest_loader = torch.utils.data.DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True,\n                                           num_workers=2,\n                                           collate_fn=custom_collate)\n\nprint(f\"Total number of train samples: {len(train_subset)}\")\nprint(f\"Total number of test samples: {len(test_subset)}\")\nprint(f\"Total number of val samples: {len(val_subset)}\")\nprint(f\"Total number of (train) batches: {len(train_loader)}\")\nprint(f\"Total number of (test) batches: {len(test_loader)}\")\nprint(f\"Total number of (val) batches: {len(val_loader)}\")","metadata":{"execution":{"iopub.status.busy":"2022-07-06T20:39:48.257064Z","iopub.execute_input":"2022-07-06T20:39:48.257434Z","iopub.status.idle":"2022-07-06T20:39:48.275289Z","shell.execute_reply.started":"2022-07-06T20:39:48.257403Z","shell.execute_reply":"2022-07-06T20:39:48.274040Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"### Visualization","metadata":{}},{"cell_type":"code","source":"!pip install imageio-ffmpeg\nimport imageio\nfrom IPython.display import Video\nimport numpy as np\n\nfor i, (fm,lb) in enumerate(train_loader):\n    print(\"Dataset batch shape\", fm.size(), fm.dtype, lb.size(),  lb.dtype )\n    # break\n    video = fm[0].permute(0, 2, 3, 1).numpy() \n    # print(video.min(), video.max(), video.mean(), video.std())\n    video = (video * 255.0).astype(np.uint8)\n    label = train_dataset.classes[lb[0]]\n    \n    print(\"Example:\", label)\n    imageio.mimwrite('./test.mp4', video, fps=30)\n    break\n\nVideo('./test.mp4', width=256, height=256, embed=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-06T20:39:49.289301Z","iopub.execute_input":"2022-07-06T20:39:49.291732Z","iopub.status.idle":"2022-07-06T20:40:00.818751Z","shell.execute_reply.started":"2022-07-06T20:39:49.291689Z","shell.execute_reply":"2022-07-06T20:40:00.817701Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"## Training Pipeline","metadata":{}},{"cell_type":"code","source":"torch.cuda.empty_cache()\n!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2022-07-06T20:40:00.821185Z","iopub.execute_input":"2022-07-06T20:40:00.821471Z","iopub.status.idle":"2022-07-06T20:40:01.613333Z","shell.execute_reply.started":"2022-07-06T20:40:00.821443Z","shell.execute_reply":"2022-07-06T20:40:01.611981Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"from torch import optim\nfrom tqdm import tqdm\nimport copy\n\n# Defining model and training options\nmodel = model.to(device=DEVICE)\n\nN_EPOCHS = 100\nLR = 0.1\nMODEL_PATH = 'best_model_ucf101_x128_sub_30'\nbest_val_acc = 0\nbest_model = None\npretrain = False\n\n# Training loop\noptimizer = optim.AdamW(model.parameters(), lr=LR) #, weight_decay=0.1)\ncriterion = nn.CrossEntropyLoss(reduction='mean')\n\nif pretrain:\n    checkpoint = torch.load(f\"./{MODEL_PATH}.pt\")\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n\nfor epoch in tqdm(range(N_EPOCHS), desc=\"Total Training Done:\", leave=False):\n        # TRAINING\n        train_loss = 0.0\n        count = 0\n        model.train()\n        t_loader = iter(train_loader)\n        t_correct, t_total = 0, 0\n        for batch in tqdm(range(len(train_loader)), desc=f\"Epoch {epoch + 1}\", position=0):\n            x, y = next(t_loader)\n            x, y = x.to(DEVICE), y.to(DEVICE)\n            y_hat = model(x)\n            loss = criterion(y_hat, y) #/ len(x)\n\n            train_loss += loss.detach().cpu().item()\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            t_correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n            t_total += len(x)\n            \n            count+=1\n            #if count>3000:\n            #    break\n        train_acc = t_correct / t_total * 100\n        \n        # TODO: LR DECAY\n        \n        # VALIDATION\n        val_loss = 0.0\n        v_correct, v_total = 0, 0\n        model.eval()\n        with torch.no_grad():\n            v_loader = iter(val_loader)\n            for batch in tqdm(range(len(val_loader)), desc=\"Validating\", position=0):\n                x, y = next(v_loader)\n                x, y = x.to(DEVICE), y.to(DEVICE)\n                y_hat = model(x)\n                loss = criterion(y_hat, y) #/ len(x)\n                val_loss += loss.detach().cpu().item()\n\n                v_correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n                v_total += len(x)\n            \n            val_acc = v_correct / v_total * 100\n            if(best_val_acc < val_acc):\n                best_model = copy.deepcopy(model)\n                \n                best_val_acc = val_acc\n                print(f\"Best accuracy: {best_val_acc}\")\n                torch.save({\n                    'epoch': epoch,\n                    'model_state_dict': best_model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict()\n                }, f\"./{MODEL_PATH}.pt\")\n                \n                best_model = best_model.cpu()\n                torch.cuda.empty_cache()\n                \n        print(f\"Epoch {epoch + 1}/{N_EPOCHS} train loss: {train_loss:.2f}, train acc {train_acc:.2f} val loss: {val_loss:.2f}\")\n        print(f\"Val accuracy: {val_acc:.2f}%\")\n        \n\n","metadata":{"execution":{"iopub.status.busy":"2022-07-06T20:41:59.027728Z","iopub.execute_input":"2022-07-06T20:41:59.028077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del best_model, model, x, y, y_hat, loss, v_correct\ngc.collect()\ntorch.cuda.empty_cache()\n!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2022-07-06T20:41:30.039806Z","iopub.execute_input":"2022-07-06T20:41:30.040242Z","iopub.status.idle":"2022-07-06T20:41:31.510537Z","shell.execute_reply.started":"2022-07-06T20:41:30.040207Z","shell.execute_reply":"2022-07-06T20:41:31.509380Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"# val_acc, val_loss, train_loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import gc\n# # del model\n# del best_model\n# gc.collect()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test loop on latest model\n# torch.cuda.empty_cache()\ncorrect, total = 0, 0\ntest_loss = 0.0\nmodel.eval()\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Testing\", position=0, leave=True):\n        x, y = batch\n        x, y = x.to(DEVICE), y.to(DEVICE)\n        y_hat = model(x)\n        loss = criterion(y_hat, y) # / len(x)\n        test_loss += loss.detach().cpu().item()\n\n        correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n        total += len(x)\n\n        if total > 1000:\n            break\n    \nprint(\"Latest Model\")\nprint(f\"Test loss: {test_loss:.2f}\")\nprint(f\"Test accuracy: {correct / total * 100:.2f}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test loop on best model\n# torch.cuda.empty_cache()\n# test_model = ConvAcTransformer(d_model=512, attention_heads=4, num_layers=4, num_classes=101, feature_extractor_name='efficientnet_v2_s')\n# test_model = test_model.to(device=DEVICE)\n# MODEL_PATH = 'best_model_ucf101_x128'\n\n# criterion = nn.CrossEntropyLoss()\n# checkpoint = torch.load(f\"./{MODEL_PATH}.pt\")\n# test_model.load_state_dict(checkpoint['model_state_dict'])\n\ncorrect, total = 0, 0\ntest_loss = 0.0\nbest_model.eval()\nfor batch in tqdm(test_loader, desc=\"Testing\", position=0, leave=True):\n    x, y = batch\n    x, y = x.to(DEVICE), y.to(DEVICE)\n    y_hat = best_model(x)\n    loss = criterion(y_hat, y) # / len(x)\n    test_loss += loss.detach().cpu().item()\n\n    correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n    total += len(x)\n\n    if total > 17:\n        break\n    \nprint(\"Best Model\")\nprint(f\"Test loss: {test_loss:.2f}\")\nprint(f\"Test accuracy: {correct / total * 100:.2f}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}